from __future__ import annotations

import io, os, re, time, math, csv
from pathlib import Path
from urllib.parse import urlparse
from typing import List, Optional

import pandas as pd
import streamlit as st

# interne moduler (forventes at findes)
import db
import charts as ch
from crawler import crawl_iter, scan_pages, DEFAULT_KW

st.set_page_config(page_title="NIRAS greenwashing-dashboard", layout="wide")

# -------------------- Helpers --------------------
SETTINGS_PATH = Path("data") / "settings.json"

def _load_settings() -> dict:
    try:
        if SETTINGS_PATH.exists():
            return pd.read_json(SETTINGS_PATH).to_dict() or {}
    except Exception:
        pass
    return {}

def _save_settings(obj: dict):
    try:
        SETTINGS_PATH.parent.mkdir(parents=True, exist_ok=True)
        pd.Series(obj).to_json(SETTINGS_PATH)
    except Exception:
        pass

def _canon(u: str, base: str | None = None) -> str:
    u = (str(u) or "").strip()
    if not u:
        return ""
    if u.startswith("/") and base:
        u = base.rstrip("/") + u
    p = urlparse(u)
    if not p.scheme or not p.netloc:
        return ""
    u = p._replace(fragment="").geturl()
    if not u.endswith("/"):
        u += "/"
    return u

def _progress_bar(completion: float, total: int, done: int):
    pct = max(0, min(int(round((completion or 0.0) * 100)), 100))
    st.progress(pct / 100.0, text=f"Fremskridt: {pct}%  ({done} af {total} sider)")

# -------------------- Header + DB init --------------------
st.title("Velkommen til Greenwashing-radaren")
st.caption("FiltrÃ©r, redigÃ©r og find forekomster hurtigt. Navigation/related tÃ¦lles ikke med i forekomster.")

db.init_db()

# Valgfri auto-import fra fil (nÃ¥r DB er tom)
AUTO_IMPORT = os.environ.get("AUTO_IMPORT_FILE", "data/crawl.csv")
def _read_any(path: str) -> pd.DataFrame:
    p = Path(path)
    if not p.exists(): return pd.DataFrame()
    if p.suffix.lower() in (".xlsx", ".xls"):
        return pd.read_excel(p)
    return pd.read_csv(p)

try:
    s0 = db.stats()
    if s0.get("total", 0) == 0 and Path(AUTO_IMPORT).exists():
        df0 = _read_any(AUTO_IMPORT)
        if not df0.empty:
            db.sync_pages_from_df(df0)
except Exception:
    pass

s0 = db.stats()
_progress = s0.get("completion", 0.0)
_progress_bar(_progress, s0.get("total", 0), s0.get("done", 0))

# -------------------- Sidebar --------------------
with st.sidebar:
    st.header("Data")

    # Normalisering af "wide" Excel (fjerner 'antal_forekomster' m.fl. som keyword)
    META_PAT = re.compile(r"(total|sum|matches?|forekomst(er)?|antal(_)?forekomster?)$", re.I)

    def normalize_wide(df: pd.DataFrame) -> pd.DataFrame | None:
        if df is None or df.empty:
            return None
        cols_lower = {str(c).strip().lower(): c for c in df.columns}
        url_col = cols_lower.get("url")
        if not url_col:
            return None

        def _is_num(s):
            try: return pd.api.types.is_numeric_dtype(s)
            except: return False

        total_col = None
        for c in df.columns:
            name = str(c).strip().lower()
            if META_PAT.fullmatch(name) or META_PAT.search(name):
                total_col = c
                break

        numeric_cols = [c for c in df.columns if c != url_col and _is_num(df[c])]
        kw_cols = [c for c in numeric_cols if not META_PAT.search(str(c).strip().lower())]

        if not kw_cols and total_col is None:
            return None

        tmp = df.copy()
        if total_col:
            total_series = pd.to_numeric(tmp[total_col], errors="coerce").fillna(0).astype(int)
        else:
            total_series = pd.to_numeric(tmp[kw_cols].fillna(0)).sum(axis=1).astype(int)

        def mk_keywords_csv(row):
            used = []
            for c in kw_cols:
                try: v = int(row.get(c, 0) or 0)
                except: v = 0
                if v > 0:
                    used.append(str(c))
            return ",".join(used)

        keywords_csv = tmp.apply(mk_keywords_csv, axis=1) if kw_cols else pd.Series([""] * len(tmp))
        hits_series = total_series

        return pd.DataFrame({
            "url": tmp[url_col].astype(str),
            "keywords": keywords_csv,
            "hits": hits_series,
            "total": total_series,
        })

    # VÃ¦lg/Upload fil
    default_path = os.path.join("data", "crawl.csv")
    path_str = st.text_input("Sti til CSV/Excel", value=default_path)
    uploaded = st.file_uploader("...eller upload fil", type=["csv", "xlsx", "xls"])
    file_source = uploaded if uploaded else (path_str if path_str.strip() else None)

    df_std, label = None, "Ingen"
    if file_source:
        try:
            if uploaded is not None:
                label = uploaded.name
                raw = uploaded.getvalue()
            else:
                label = str(path_str)
                raw = Path(path_str).read_bytes() if Path(path_str).exists() else b""

            if raw:
                df_try = None
                if label.lower().endswith((".xlsx", ".xls")):
                    for kwargs in ({"engine": None}, {"engine": "openpyxl"}):
                        try:
                            df_try = pd.read_excel(io.BytesIO(raw), **{k: v for k, v in kwargs.items() if v is not None})
                            if df_try is not None and not df_try.empty: break
                        except Exception: df_try = None
                if df_try is None:
                    for kwargs in (
                        {"engine": "python", "encoding": "utf-8", "comment": "#", "on_bad_lines": "skip"},
                        {"sep": ";", "engine": "python", "encoding": "utf-8", "comment": "#", "on_bad_lines": "skip"},
                        {"sep": ",", "engine": "python", "encoding": "utf-8", "comment": "#", "on_bad_lines": "skip"},
                    ):
                        try:
                            df_try = pd.read_csv(io.BytesIO(raw), **kwargs)
                            if df_try is not None and not df_try.empty: break
                        except Exception: df_try = None

                if df_try is not None and not df_try.empty:
                    cols_l = {c.lower() for c in df_try.columns}
                    if {"url", "keywords", "hits", "total"}.issubset(cols_l):
                        df_std = df_try.rename(columns={c: c.lower() for c in df_try.columns})
                    else:
                        nw = normalize_wide(df_try)
                        df_std = nw if (nw is not None and not nw.empty) else pd.DataFrame()
                else:
                    st.warning("Kunne ikke lÃ¦se filen.")
        except Exception as e:
            st.error(f"Fejl ved indlÃ¦sning: {e}")
            df_std = None

    st.caption(f"Datakilde: {label}")

    # Import (upsert kun kw/hits/total â€“ bevar status/notes/assigned)
    if st.button("ImportÃ©r", type="primary"):
        if df_std is None or df_std.empty:
            st.warning("Ingen gyldig data fundet.")
            st.stop()

        base = st.session_state.get("__current_domain")
        df_imp = df_std.copy()
        for c in ("url", "keywords", "hits", "total"):
            if c not in df_imp.columns:
                df_imp[c] = "" if c in ("url", "keywords") else 0

        df_imp["url"] = df_imp["url"].map(lambda u: _canon(u, base))
        df_imp = df_imp[df_imp["url"] != ""]
        df_imp["keywords"] = df_imp["keywords"].fillna("").astype(str)
        df_imp["hits"] = pd.to_numeric(df_imp["hits"], errors="coerce").fillna(0).astype(int)
        df_imp["total"] = pd.to_numeric(df_imp["total"], errors="coerce").fillna(0).astype(int)

        rows = df_imp.to_dict("records")
        BATCH = 500
        synced = 0
        for i in range(0, len(rows), BATCH):
            chunk = pd.DataFrame(rows[i:i+BATCH])
            tries = 0
            while True:
                try:
                    db.sync_pages_from_df(chunk); synced += len(chunk); break
                except Exception:
                    tries += 1
                    if tries >= 3:
                        st.error("DB-fejl ved import.")
                        break
                    st.warning(f"DB-fejl (forsÃ¸g {tries}). PrÃ¸ver igen om 1sâ€¦")
                    time.sleep(1)
        st.success(f"Import fÃ¦rdig. {synced} rÃ¦kker synkroniseret.")
        st.rerun()

    st.divider()
    st.header("Crawler")

    domain = st.selectbox("DomÃ¦ne", ["https://www.niras.dk/", "https://www.niras.com/"])
    st.session_state["__current_domain"] = domain

    default_kw_text = "\n".join(DEFAULT_KW)
    kw_text = st.text_area(
        "SÃ¸geord & udsagn (Ã©t pr. linje)",
        value=default_kw_text,
        help="* som wildcard (fx 'bÃ¦redygtig*'). Regex som /co2[- ]?neutral/."
    )
    kw_list_manual = [k.strip() for k in re.split(r"[\n,;]", kw_text) if k.strip()]

    merge_with_file = st.checkbox("Flet med keywords fra datakilden", value=True)
    kw_from_file = []
    if merge_with_file and (df_std is not None) and (not df_std.empty):
        try:
            all_kw = []
            for _, row in df_std.iterrows():
                for k in re.split(r"[;,]", str(row.get("keywords", ""))):
                    k = k.strip()
                    if k:
                        all_kw.append(k)
            seen = set()
            kw_from_file = [k for k in all_kw if not (k in seen or seen.add(k))]
        except Exception:
            kw_from_file = []

    kw_seen, kw_final = set(), []
    for k in kw_list_manual + kw_from_file:
        if k and (k not in kw_seen):
            kw_seen.add(k); kw_final.append(k)

    st.caption("â€”")
    settings = _load_settings()
    exclude_text = st.text_area(
        "EkskludÃ©r ord/fraser (Ã©t pr. linje)",
        value="\n".join(settings.get("exclude", [])),
        help="Filtreres vÃ¦k fra brÃ¸dtekst (ikke fra keyword-listen).",
        key="exclude_kw_text"
    )
    kw_exclude = {k.strip().lower() for k in re.split(r"[\n,;]", exclude_text) if k.strip()}
    st.session_state["kw_final"] = kw_final
    st.session_state["kw_exclude"] = sorted(list(kw_exclude)) if kw_exclude else []
    excl_sig = (exclude_text or "").strip()
    if st.session_state.get("__exclude_sig") != excl_sig:
        st.session_state["__exclude_sig"] = excl_sig
        _save_settings({"exclude": [k for k in excl_sig.split("\n") if k.strip()]})
        st.rerun()

    st.caption(f"Keywords i brug: {len(kw_final)}")

    if st.button("ðŸš€ Crawl hele domÃ¦net", type="secondary"):
        if not kw_final:
            st.warning("TilfÃ¸j mindst Ã©t ord/udsagn.")
        else:
            db.init_db()
            prog = st.progress(0.0, text="Starter crawlerâ€¦")
            rows_buf, BATCH, db_errors = [], 150, 0

            def on_progress(done: int, queued: int):
                pct = min(0.99, done / 8000)
                prog.progress(pct, text=f"Crawlerâ€¦ {done} sider Â· kÃ¸: {queued}")

            for row in crawl_iter(
                domain, kw_final,
                max_pages=12000, max_depth=100, delay=0.5,
                progress_cb=on_progress, excludes=st.session_state.get("kw_exclude", [])
            ):
                rows_buf.append(row)
                if len(rows_buf) >= BATCH:
                    try:
                        db.sync_pages_from_df(pd.DataFrame(rows_buf))
                        rows_buf.clear(); db_errors = 0
                    except Exception:
                        db_errors += 1
                        st.warning(f"DB-fejl (forsÃ¸g {db_errors}). PrÃ¸ver igen om 1sâ€¦")
                        time.sleep(1)
                        try:
                            db.sync_pages_from_df(pd.DataFrame(rows_buf))
                            rows_buf.clear(); db_errors = 0
                        except Exception:
                            pass

            if rows_buf:
                try:
                    db.sync_pages_from_df(pd.DataFrame(rows_buf))
                except Exception:
                    st.warning("Kunne ikke skrive sidste batch; prÃ¸ver igenâ€¦")
                    time.sleep(1)
                    try:
                        db.sync_pages_from_df(pd.DataFrame(rows_buf))
                    except Exception as e:
                        st.error(f"Kunne ikke skrive sidste batch: {e}")

            prog.progress(1.0, text="Crawler fÃ¦rdig")
            st.success("Crawl fÃ¦rdig."); st.rerun()

    st.subheader("Crawl fra fil (URL-liste)")
    urls_file = st.file_uploader("Upload .txt eller .csv med URL'er", type=["txt", "csv"], key="url_list")
    if urls_file is not None:
        raw = urls_file.getvalue().decode("utf-8", errors="ignore")
        urls = []
        if urls_file.name.lower().endswith(".csv"):
            rdr = csv.DictReader(io.StringIO(raw))
            fld_l = [f.strip().lower() for f in (rdr.fieldnames or [])]
            url_field = None
            if "url" in fld_l:
                url_field = rdr.fieldnames[fld_l.index("url")]
            if url_field:
                for row in rdr:
                    u = (row.get(url_field) or "").strip()
                    if u: urls.append(u)
            else:
                urls = [row[0].strip() for row in csv.reader(io.StringIO(raw)) if row and row[0].strip()]
        else:
            urls = [ln.strip() for ln in raw.splitlines() if ln.strip()]
        base = st.session_state.get("__current_domain")
        urls = [_canon(u, base) for u in urls]
        urls = [u for u in urls if u]
        urls = list(dict.fromkeys(urls))
        st.caption(f"Fundet {len(urls)} URL'er i filen.")
        if urls and st.button("Crawl uploadede URL'er nu", type="secondary"):
            st.info(f"Crawler {len(urls)} URL'er â€¦")
            sub_prog = st.progress(0.0)
            batch = 40
            all_rows = []
            kw_final = st.session_state.get("kw_final", [])
            kw_excl = st.session_state.get("kw_exclude", [])
            from math import ceil
            total_batches = max(1, ceil(len(urls) / batch))
            for i in range(0, len(urls), batch):
                part = urls[i:i + batch]
                try:
                    part_rows = scan_pages(part, kw_final, excludes=kw_excl, delay=0.0)
                    if part_rows:
                        db.sync_pages_from_df(pd.DataFrame(part_rows))
                        all_rows.extend(part_rows)
                except Exception as e:
                    st.warning(f"Fejl ved batch {i // batch + 1}: {e}")
                sub_prog.progress(min(1.0, (i // batch + 1) / total_batches))
            st.success(f"FÃ¦rdig. Opdateret {len(all_rows)} resultater i DB.")
            st.rerun()

    # GA Top 100
    st.divider()
    st.header("Google Analytics â€“ Top 100")
    ga_file = st.file_uploader("Upload GA CSV/Excel (URL/pagePath + pageviews)", type=["csv", "xlsx", "xls"], key="ga_csv")
    raw: bytes = b""
    src_name: str = ""
    if ga_file is not None:
        src_name = (ga_file.name or "").lower()
        raw = ga_file.getvalue() or b""
    else:
        default_ga_path = Path("data") / "Pageviews.csv"
        if default_ga_path.exists():
            src_name = str(default_ga_path).lower()
            try:
                raw = default_ga_path.read_bytes()
            except Exception:
                raw = b""

    if raw:
        ga_df = None
        name = src_name
        is_excel = name.endswith(".xlsx") or name.endswith(".xls")
        if is_excel:
            for kwargs in ({"engine": None}, {"engine": "openpyxl"}):
                try:
                    ga_df = pd.read_excel(io.BytesIO(raw), **{k: v for k, v in kwargs.items() if v is not None})
                    if ga_df is not None and not ga_df.empty: break
                except Exception: ga_df = None
        if ga_df is None or ga_df.empty:
            for kwargs in (
                {"engine": "python", "encoding": "utf-8", "comment": "#", "on_bad_lines": "skip"},
                {"sep": ";", "engine": "python", "encoding": "utf-8", "comment": "#", "on_bad_lines": "skip"},
                {"sep": ",", "engine": "python", "encoding": "utf-8", "comment": "#", "on_bad_lines": "skip"},
            ):
                try:
                    ga_df = pd.read_csv(io.BytesIO(raw), **kwargs)
                    if ga_df is not None and not ga_df.empty: break
                except Exception: ga_df = None

        if ga_df is not None and not ga_df.empty:
            def _norm(s: str) -> str:
                return re.sub(r"[^a-z]", "", (str(s) or "").strip().lower())

            by_lower = {str(c).strip().lower(): c for c in ga_df.columns}
            by_norm = {_norm(c): c for c in ga_df.columns}

            url_keys = ["url", "pagepath", "page", "pagelocation", "landingpage", "landingpagepath", "pathname", "pagepathandscreenclass"]
            pv_keys = ["pageviews", "views", "screenpageviews", "screenpageview", "screenviews"]

            url_col = None
            for k in url_keys:
                url_col = by_lower.get(k) or by_norm.get(k)
                if url_col: break
            if not url_col:
                for nk, orig in by_norm.items():
                    if ("pagepath" in nk) or ("pagelocation" in nk) or (nk == "url"):
                        url_col = orig; break

            pv_col = None
            for k in pv_keys:
                pv_col = by_lower.get(k) or by_norm.get(k)
                if pv_col: break
            if not pv_col:
                for nk, orig in by_norm.items():
                    if nk.endswith("views") or ("pageviews" in nk) or ("screenpageviews" in nk):
                        pv_col = orig; break

            if url_col and pv_col:
                def canon_ga(u: str) -> str:
                    base = st.session_state.get("__current_domain") or ""
                    return _canon(u, base)
                ga_df = ga_df.rename(columns={url_col: "ga_url", pv_col: "pageviews"})
                ga_df["url"] = ga_df["ga_url"].map(canon_ga)
                ga_df["pageviews"] = pd.to_numeric(ga_df["pageviews"], errors="coerce").fillna(0).astype(int)
                ga_top = ga_df.sort_values("pageviews", ascending=False).head(100).copy()
                st.session_state["ga_top100"] = ga_top[["url", "pageviews"]]
                st.success("IndlÃ¦st GA top 100. Se fanen 'Fokus (Top 100)'.")
            else:
                st.warning("GA-fil mangler URL/pagePath og pageviews.")

# -------------------- Tabs --------------------
tab_overview, tab_stats, tab_done, tab_review, tab_focus = st.tabs(
    ["Oversigt", "Statistik", "FÃ¦rdige sider", "Needs Review", "Fokus (Top 100)"]
)

# Oversigt
with tab_overview:
    st.subheader("Oversigt")
    c1, c2, c3 = st.columns([2, 1, 1])
    q = c1.text_input("SÃ¸g (URL/keywords)", value="")
    min_total = c2.number_input("Min. total", min_value=0, value=0, step=1)
    try:
        status_choice = c3.segmented_control("Status", options=["Alle", "Todo", "Needs Review", "Done"], default="Alle")
    except Exception:
        status_choice = c3.selectbox("Status", ["Alle", "Todo", "Needs Review", "Done"], index=0)
    status_arg = {"Alle": None, "Todo": "todo", "Needs Review": "review", "Done": "done"}[status_choice]

    rows, total_count = db.get_pages(
        search=q.strip() or None, min_total=int(min_total), status=status_arg,
        sort_by="total", sort_dir="desc", limit=10000, offset=0
    )
    st.caption(f"Viser {len(rows)} af {total_count} sider")

    if not rows:
        st.info("Ingen sider matcher filtrene.")
    else:
        df = pd.DataFrame([dict(r) for r in rows])
        for col, default in [("url",""),("keywords",""),("hits",0),("total",0),("status","todo"),("notes",""),("assigned_to","")]:
            if col not in df.columns: df[col] = default
        df["URL"] = df["url"]
        df["Keywords"] = df["keywords"].fillna("")
        df["Hits"] = pd.to_numeric(df["hits"], errors="coerce").fillna(0).astype(int)
        df["Total"] = pd.to_numeric(df["total"], errors="coerce").fillna(0).astype(int)
        df["Status"] = df["status"].map({"todo":"Todo","done":"Done","review":"Needs Review"}).fillna("Todo")
        df["Assigned to"] = df["assigned_to"].fillna("").replace({None:""})
        df["Noter"] = df["notes"].fillna("")
        view = df[["URL","Keywords","Hits","Total","Status","Assigned to","Noter"]]
        edited = st.data_editor(
            view, width="stretch", hide_index=True,
            column_config={
                "URL": st.column_config.LinkColumn(help="Ã…bn"),
                "Keywords": st.column_config.TextColumn(width="large"),
                "Hits": st.column_config.NumberColumn(format="%d"),
                "Total": st.column_config.NumberColumn(format="%d"),
                "Status": st.column_config.SelectboxColumn(options=["Todo","Needs Review","Done"]),
                "Assigned to": st.column_config.SelectboxColumn(options=["â€“ Ingen â€“","RAGL","CEYD","ULRS","LBY","JAWER"]),
                "Noter": st.column_config.TextColumn(),
            },
            disabled=["URL","Keywords","Hits","Total"],
            key="overview_editor",
            on_change=lambda: st.session_state.update({"overview_changed": True}),
        )
        if st.session_state.get("overview_changed", False):
            changed = 0
            for i, row in edited.iterrows():
                orig = df.loc[i]; url = orig["URL"]
                if row["Status"] != orig["Status"]:
                    db.update_status(url, {"Todo":"todo","Done":"done","Needs Review":"review"}[row["Status"]]); changed += 1
                if row["Noter"] != orig["Noter"]:
                    db.update_notes(url, row["Noter"]); changed += 1
                new_assign = "" if row["Assigned to"] == "â€“ Ingen â€“" else row["Assigned to"]
                if new_assign != orig["Assigned to"]:
                    db.update_assigned_to(url, new_assign); changed += 1
            if changed:
                st.success(f"GEMT: {changed} Ã¦ndring(er)")
                st.session_state["overview_changed"] = False
                time.sleep(1.0); st.rerun()

# Statistik
with tab_stats:
    st.subheader("Statistik & Progress")
    s = db.stats()
    ch.kpi_cards(s.get("total",0), s.get("done",0), s.get("todo",0), s.get("completion",0.0))

    rows, _ = db.get_pages(limit=100000, offset=0)
    df_all = pd.DataFrame([dict(r) for r in rows]) if rows else pd.DataFrame(columns=["url","keywords","total"])

    def _split_csv(s): return [k.strip() for k in str(s or "").split(",") if k.strip()]

    if not df_all.empty:
        explode = df_all.assign(_kw=df_all["keywords"].map(_split_csv)).explode("_kw")
        explode = explode[explode["_kw"].notna() & (explode["_kw"]!="")]
        pages_per_kw = explode.groupby("_kw")["url"].nunique().sort_values(ascending=False).head(15)
        totals_per_kw = explode.groupby("_kw")["url"].size().sort_values(ascending=False).head(15)
    else:
        pages_per_kw = pd.Series(dtype=int); totals_per_kw = pd.Series(dtype=int)

    c1, c2 = st.columns(2)
    with c1:
        st.markdown("**Sider pr. keyword**")
        if pages_per_kw.empty: st.info("Ingen data endnu.")
        else: ch.bar_keyword_pages(pages_per_kw)
    with c2:
        st.markdown("**Top-keywords (forekomster, est.)**")
        if totals_per_kw.empty: st.info("Ingen data endnu.")
        else: ch.bar_keyword_totals(totals_per_kw)

# FÃ¦rdige
with tab_done:
    st.subheader("FÃ¦rdige sider")
    done_df = db.get_done_dataframe()
    if done_df.empty:
        st.info("Ingen fÃ¦rdige sider endnu.")
    else:
        st.dataframe(done_df, use_container_width=True, hide_index=True)
        st.download_button("EksportÃ©r CSV", data=done_df.to_csv(index=False).encode("utf-8"),
                           file_name="faerdige_sider.csv", mime="text/csv")
        undo = st.multiselect("Fortryd til Todo", options=list(done_df.get("url", [])))
        if st.button("Fortryd valgte"):
            if undo: db.bulk_update_status(undo, "todo"); st.success("Status opdateret."); st.rerun()
            else: st.info("VÃ¦lg mindst Ã©n URL.")

# Needs Review
with tab_review:
    st.subheader("Sider der krÃ¦ver ekstra opmÃ¦rksomhed")
    review_rows, _ = db.get_pages(status="review", limit=100000, offset=0)
    review_df = pd.DataFrame([dict(r) for r in review_rows]) if review_rows else pd.DataFrame()
    if review_df.empty:
        st.info("Ingen sider markeret som 'Needs Review'.")
    else:
        for col, default in [("url",""),("keywords",""),("total",0),("assigned_to",""),("notes","")]:
            if col not in review_df.columns: review_df[col] = default
        review_df["URL"] = review_df["url"]
        review_df["Keywords"] = review_df["keywords"].fillna("")
        review_df["Total"] = pd.to_numeric(review_df["total"], errors="coerce").fillna(0).astype(int)
        review_df["Assigned to"] = review_df["assigned_to"].fillna("").replace({None:""})
        review_df["Noter"] = review_df["notes"].fillna("")
        st.dataframe(review_df[["URL","Keywords","Total","Assigned to","Noter"]],
                     use_container_width=True, hide_index=True)
        res = st.multiselect("MarkÃ©r som Done", options=list(review_df["url"]))
        back = st.multiselect("Send til Todo", options=list(review_df["url"]), key="rv_todo")
        c1, c2 = st.columns(2)
        with c1:
            if st.button("MarkÃ©r valgte som Done"):
                if res: db.bulk_update_status(res, "done"); st.success(f"{len(res)} sider markeret som Done."); st.rerun()
        with c2:
            if st.button("Send valgte til Todo"):
                if back: db.bulk_update_status(back, "todo"); st.success(f"{len(back)} sider sendt til Todo."); st.rerun()

# Fokus (Top 100)
with tab_focus:
    st.subheader("Google Analytics Top 100 â€“ fokusliste")
    ga_top = st.session_state.get("ga_top100")
    if ga_top is None or len(ga_top) == 0:
        st.info("Upload en GA CSV i sidebar for at se top 100.")
    else:
        rows, _ = db.get_pages(limit=100000, offset=0)
        db_df = pd.DataFrame([dict(r) for r in rows]) if rows else pd.DataFrame()
        for col, default in [("url",""),("total",0),("status","todo"),("assigned_to","")]:
            if col not in db_df.columns: db_df[col] = default

        focus = ga_top.merge(db_df[["url","total","status","assigned_to"]], on="url", how="left")
        focus["Matches (Total)"] = pd.to_numeric(focus["total"], errors="coerce").fillna(0).astype(int)
        focus["Status"] = focus["status"].fillna("todo").map({"todo":"Todo","done":"Done","review":"Needs Review"}).fillna("Todo")
        focus["Assigned to"] = focus["assigned_to"].fillna("").replace({None:""})

        done_in_ga = (focus["Status"] == "Done").sum()
        target = max(0, 100 - done_in_ga)

        df_show = focus[(focus["Status"] != "Done") & (focus["Matches (Total)"] > 0)].copy()
        if df_show.empty:
            df_show = focus[focus["Status"] != "Done"].copy()
        if df_show.empty:
            df_show = focus.copy()

        c1, c2, c3 = st.columns([2.5,1,1])
        q = c1.text_input("FiltrÃ©r i URL (substring eller regex /â€¦/)", value="", key="focus_url_q")
        prefix_mode = c2.checkbox("Starter med", value=False, key="focus_prefix")
        regex_mode = c3.checkbox("Regex", value=False, key="focus_regex")
        if q:
            if regex_mode and len(q)>=2 and q.startswith("/") and q.endswith("/"):
                try:
                    pat = re.compile(q[1:-1], re.IGNORECASE)
                    df_show = df_show[df_show["url"].astype(str).apply(lambda s: bool(pat.search(s)))]
                except Exception:
                    st.warning("Ugyldig regex â€“ bruger substring.")
                    df_show = df_show[df_show["url"].str.contains(q.strip("/"), case=False, na=False)]
            elif prefix_mode:
                def _path_starts(u: str, prefix: str) -> bool:
                    try:
                        p = urlparse(u); path = (p.path or "/")
                        return path.lower().startswith(prefix.lower())
                    except Exception: return False
                df_show = df_show[df_show["url"].astype(str).apply(lambda u: _path_starts(u, q))]
            else:
                df_show = df_show[df_show["url"].str.contains(q, case=False, na=False)]

        df_show = df_show.sort_values(["pageviews","Matches (Total)"], ascending=[False, False]).reset_index(drop=True)
        if target > 0 and len(df_show) > target:
            df_show = df_show.head(target)

        st.caption(f"GA i alt: {len(focus)} Â· Done i GA: {done_in_ga} Â· Viser: {len(df_show)}")

        df_view = df_show[["url","pageviews","Matches (Total)","Status","Assigned to"]].copy()
        df_view.insert(0, "VÃ¦lg", False)

        edited = st.data_editor(
            df_view,
            use_container_width=True, hide_index=True, height=440, key="top100_editor",
            column_config={
                "VÃ¦lg": st.column_config.CheckboxColumn(default=False),
                "url": st.column_config.LinkColumn(help="Ã…bn"),
                "pageviews": st.column_config.NumberColumn(format="%d"),
                "Matches (Total)": st.column_config.NumberColumn(format="%d"),
                "Status": st.column_config.SelectboxColumn(options=["Todo","Needs Review","Done"]),
                "Assigned to": st.column_config.SelectboxColumn(options=["â€“ Ingen â€“","CEYD","LBY","JAWER","ULRS"]),
            },
            disabled=["url","pageviews","Matches (Total)"],
            on_change=lambda: st.session_state.update({"top100_changed": True}),
        )

        if st.session_state.get("top100_changed", False):
            changed = 0
            for i, row in edited.iterrows():
                if i >= len(df_show): continue
                url = df_show.loc[i, "url"]
                if row.get("Status") != df_show.loc[i, "Status"]:
                    db.update_status(url, {"Todo":"todo","Done":"done","Needs Review":"review"}[row["Status"]]); changed += 1
                new_assign = "" if row.get("Assigned to") == "â€“ Ingen â€“" else row.get("Assigned to")
                if new_assign != df_show.loc[i, "Assigned to"]:
                    db.update_assigned_to(url, new_assign); changed += 1
            if changed:
                st.success(f"GEMT: {changed} Ã¦ndring(er)")
                st.session_state["top100_changed"] = False
                time.sleep(1.0); st.rerun()